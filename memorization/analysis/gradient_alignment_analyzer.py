import torch
from torch import nn
import torch.nn.functional as F
from contextlib import nullcontext
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel
import logging

logger = logging.getLogger(__name__)


def iter_trainable_params(model: PreTrainedModel, only_last_n_layers: Optional[int] = None) -> List[nn.Parameter]:
    """ÌïÑÏöî Ïãú ÎßàÏßÄÎßâ NÍ∞ú Î†àÏù¥Ïñ¥Îßå Í∑∏ÎùºÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞(Í∞ÄÏÜç/Î©îÎ™®Î¶¨ Ï†àÏïΩ)."""
    params = [p for p in model.parameters() if p.requires_grad]
    if only_last_n_layers is None:
        return params
    # Í∞ÑÎã®ÌïòÍ≤å ÎÅùÏóêÏÑú NÍ∞úÏùò ÌååÎùºÎØ∏ÌÑ∞ ÌÖêÏÑúÎ•º ÏÇ¨Ïö© (Î†àÏù¥Ïñ¥ Îã®ÏúÑÎ°ú Îçî Ï†ïÍµêÌôî Í∞ÄÎä•)
    return params[-only_last_n_layers:]


def _grads_to_cpu_fp32(params: List[nn.Parameter]) -> List[torch.Tensor]:
    """ÌòÑÏû¨ paramsÏùò gradÎ•º CPU FP32 ÌÖêÏÑú(ÌèâÌÉÑ) Î¶¨Ïä§Ìä∏Î°ú Î∞òÌôò."""
    cpu_grads: List[torch.Tensor] = []
    for p in params:
        if p.grad is None:
            cpu_grads.append(torch.empty(0, dtype=torch.float32))
            continue
        g_cpu = p.grad.detach().to(dtype=torch.float32, device="cpu").view(-1).contiguous()
        cpu_grads.append(g_cpu)
    return cpu_grads


def _cpu_grads_norm(cpu_grads: List[torch.Tensor]) -> float:
    norm_sq = 0.0
    for g in cpu_grads:
        if g.numel() == 0:
            continue
        g_fp32 = g.to(dtype=torch.float32)
        norm_sq += float(torch.dot(g_fp32, g_fp32))
    return float(norm_sq ** 0.5 + 1e-12)


def _dot_with_cpu_grads(params: List[nn.Parameter], cpu_grads_ref: List[torch.Tensor]) -> Tuple[float, float]:
    """ÌòÑÏû¨ params.gradÏôÄ CPUÏóê Ï†ÄÏû•Îêú Í∏∞Ï§Ä grad Í∞Ñ ÎÇ¥Ï†ÅÍ≥º ÌòÑÏû¨ grad ÎÖ∏Î¶ÑÏùÑ Í≥ÑÏÇ∞.
    - ÎÇ¥Ï†ÅÍ≥º ÎÖ∏Î¶ÑÏùÄ CPUÏóêÏÑú Í≥ÑÏÇ∞Ìï¥ GPU Î©îÎ™®Î¶¨Î•º ÏµúÏÜåÌôîÌïúÎã§.
    """
    dot_sum = 0.0
    norm_sq = 0.0
    for p, g_ref in zip(params, cpu_grads_ref):
        if p.grad is None or g_ref.numel() == 0:
            continue
        g_cpu = p.grad.detach().to(dtype=torch.float32, device="cpu").view(-1)
        g_ref_fp32 = g_ref.to(dtype=torch.float32)
        dot_sum += float(torch.dot(g_cpu, g_ref_fp32))
        norm_sq += float(torch.dot(g_cpu, g_cpu))
    return dot_sum, float(norm_sq ** 0.5 + 1e-12)


def sequence_loss(model: PreTrainedModel, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """
    labelsÎäî causal LM Í¥ÄÎ°ÄÎåÄÎ°ú input_idsÎ•º Ìïú Ïπ∏ ÏãúÌîÑÌä∏Ìï¥ Í≥ÑÏÇ∞.
    padding ÌÜ†ÌÅ∞ÏùÄ -100ÏúºÎ°ú Î¨¥Ïãú.
    """
    labels = input_ids.clone()
    labels[attention_mask == 0] = -100
    out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    # HFÎäî ÌèâÍ∑† ÌÜ†ÌÅ∞ CEÎ•º Î∞òÌôò(Î¨¥Ïãú ÌÜ†ÌÅ∞ Ï†úÏô∏) ‚Üí Ï†ïÏùò 3.1Ïùò ÏãúÌÄÄÏä§ ÌèâÍ∑† ÏÜêÏã§Í≥º Ìï©ÏπòÍ≤å ÏÇ¨Ïö©
    return out.loss


def sequence_backward(
    model: nn.Module,
    params: List[nn.Parameter],
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    fp16_autocast: bool = False,
    grad_scaler: Optional[torch.cuda.amp.GradScaler] = None,
) -> None:
    """
    ÏãúÌÄÄÏä§Ïóê ÎåÄÌïú backward pass ÏàòÌñâ
    
    Args:
        grad_scaler: FP16 ÏÇ¨Ïö© Ïãú ÏàòÏπòÏ†Å ÏïàÏ†ïÏÑ±ÏùÑ ÏúÑÌïú gradient scaler.
                    ÏùºÎ∞òÏ†ÅÏúºÎ°ú inference+backwardÏóêÏÑúÎäî ÌïÑÏöîÌïòÏßÄ ÏïäÏßÄÎßå,
                    Îß§Ïö∞ ÏûëÏùÄ Î™®Îç∏Ïù¥ÎÇò Î∞∞ÏπòÏóêÏÑú underflow ÏúÑÌóòÏù¥ ÏûàÏùÑ Ïàò ÏûàÏùå.
    """
    model.zero_grad(set_to_none=True)
    # NOTE: Í∏∞Ï°¥ full-backward Í≤ΩÎ°úÎäî ÎπÑÍ∂åÏû•(ÏÑ±Îä• ÎπÑÌö®Ïú®). Ïú†ÏßÄÎßå ÌïòÍ≥† ÏÇ¨Ïö©ÏùÄ ÏßÄÏñë.
    # ÎîîÎ∞îÏù¥Ïä§Ïóê Îî∞Î•∏ ÏïàÏ†ÑÌïú autocast Ï≤òÎ¶¨
    if input_ids.is_cuda:
        amp_ctx = torch.autocast("cuda", dtype=torch.float16, enabled=fp16_autocast)
    else:
        amp_ctx = nullcontext()
    with amp_ctx:
        loss = sequence_loss(model, input_ids, attention_mask)
    # ÏùºÎ∞òÏ†ÅÏù∏ backward (Î∂ÑÏÑù Î™©Ï†ÅÏóêÏÑ† GradScaler Î∂àÏÇ¨Ïö© Í∂åÏû•)
    loss.backward()
    
    # grads are now stored in params
    return None


@dataclass
class AlignOutputs:
    align_inner: float
    align_cosine: float
    grad_norm_orig: float
    grad_norm_para_mean: float


# ------------------------------
# Head-only gradient alignment
# ------------------------------

def _final_norm(model: PreTrainedModel, hidden: torch.Tensor) -> torch.Tensor:
    """Î™®Îç∏ Ïú†ÌòïÏóê Îî∞Îùº ÏµúÏ¢Ö LayerNormÏùÑ Ï†ÅÏö©(GPT: ln_f, LLaMA: model.norm)."""
    # LLaMA Í≥ÑÏó¥
    if hasattr(getattr(model, "model", None), "norm"):
        return model.model.norm(hidden)
    # GPT2/NeoX Í≥ÑÏó¥
    if hasattr(getattr(model, "transformer", None), "ln_f"):
        return model.transformer.ln_f(hidden)
    return hidden


def _encode(tokenizer: AutoTokenizer, texts, device: str, max_len: int):
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    enc = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=max_len)
    return enc["input_ids"].to(device), enc["attention_mask"].to(device)


@torch.no_grad()
def _backbone_hidden(model: PreTrainedModel, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """Î∞±Î≥∏ÏùÄ no_gradÎ°úÎßå ÏÇ¨Ïö©, ÎßàÏßÄÎßâ ÌûàÎì†ÏùÑ Ï∂îÏ∂ú ÌõÑ ÌïÑÏöî Ïãú ÏµúÏ¢Ö Norm Ï†ÅÏö©."""
    out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, use_cache=False)
    hidden_last = out.hidden_states[-1]
    return _final_norm(model, hidden_last)


def _masked_ce(logits: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """Causal LM ÏãúÌîÑÌä∏ CEÎ•º ÎßàÏä§ÌÅ¨ ÌèâÍ∑†ÏúºÎ°ú Í≥ÑÏÇ∞(HFÏùò Ïú†Ìö® ÌÜ†ÌÅ∞ ÌèâÍ∑†Í≥º ÏùºÏπò)."""
    tgt = input_ids[:, 1:].contiguous()
    mask = attention_mask[:, 1:].contiguous()
    logits_use = logits[:, :-1, :].contiguous()
    v = logits_use.size(-1)
    loss_tok = F.cross_entropy(logits_use.view(-1, v), tgt.view(-1), reduction="none")
    loss_tok = loss_tok.view_as(tgt)
    denom = mask.sum().clamp_min(1)
    return (loss_tok * mask).sum() / denom


def _grad_head_vector(
    model: PreTrainedModel,
    hidden: torch.Tensor,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    use_autocast: bool,
) -> Tuple[torch.Tensor, float]:
    """Ï∂úÎ†• Ìó§Îìú ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ÏÑúÎßå Í∑∏ÎùºÎîîÏñ∏Ìä∏ Î≤°ÌÑ∞Î•º Í≥ÑÏÇ∞ÌïòÍ≥† CPU FP32Î°ú Î∞òÌôò."""
    head = model.get_output_embeddings()
    if head is None:
        # fallback (ÏùºÎ∂Ä Î™®Îç∏ÏùÄ lm_headÎßå Ï°¥Ïû¨)
        head = getattr(model, "lm_head", None)
    if head is None:
        raise RuntimeError("Output head (lm_head) moduleÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")

    params = [p for p in head.parameters()]
    # ÏïàÏ†ÑÌïú autocast (CUDAÏóêÏÑúÎßå)
    amp_ctx = torch.autocast("cuda", dtype=torch.float16, enabled=use_autocast) if hidden.is_cuda else nullcontext()
    with amp_ctx:
        logits = head(hidden)  # [B, T, V]
        loss = _masked_ce(logits, input_ids, attention_mask)

    grads = torch.autograd.grad(loss, params, retain_graph=False, allow_unused=True)
    flat = []
    for g in grads:
        if g is None:
            continue
        flat.append(g.detach().to("cpu", dtype=torch.float32).view(-1))
    g_flat = torch.cat(flat, dim=0) if flat else torch.zeros(0, dtype=torch.float32)
    norm = float(torch.linalg.vector_norm(g_flat).item() + 1e-12)
    return g_flat, norm


def paraphrase_alignment_for_one(
    model: PreTrainedModel,
    tokenizer: AutoTokenizer,
    orig_text: str,
    paraphrases: List[str],
    params: Optional[List[nn.Parameter]] = None,  # deprecated in head-only path
    max_len: int = 512,
    device: str = "cuda",
    use_cosine: bool = True,
    fp16_autocast: bool = False,
) -> AlignOutputs:
    """Ìó§Îìú(Ï∂úÎ†•Ï∏µ) ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ÏÑúÎßå Í∑∏ÎùºÎîîÏñ∏Ìä∏ Ï†ïÎ†¨ÏùÑ Í≥ÑÏÇ∞(Ìö®Ïú® Î™®Îìú)."""
    # ÏõêÎ≥∏ ÌûàÎì† (Î∞±Î≥∏ÏùÄ no_grad)
    input_ids, attn = _encode(tokenizer, [orig_text], device, max_len)
    hidden = _backbone_hidden(model, input_ids, attn)
    g_ref, ref_norm = _grad_head_vector(model, hidden, input_ids, attn, fp16_autocast)

    inner_sum = 0.0
    cosine_sum = 0.0
    para_norm_sum = 0.0
    for p_text in paraphrases:
        input_ids_p, attn_p = _encode(tokenizer, [p_text], device, max_len)
        hidden_p = _backbone_hidden(model, input_ids_p, attn_p)
        g_p, p_norm = _grad_head_vector(model, hidden_p, input_ids_p, attn_p, fp16_autocast)

        # ÎÇ¥Ï†Å/ÏΩîÏÇ¨Ïù∏ Í≥ÑÏÇ∞ (CPU FP32)
        # Í∏∏Ïù¥ Î∂àÏùºÏπò Í∞ÄÎä•ÏÑ±ÏùÄ ÎÇÆÏßÄÎßå ÏïàÏ†ÑÌïòÍ≤å Í≥µÌÜµ Í∏∏Ïù¥Î°ú Ï≤òÎ¶¨
        min_len = min(g_ref.numel(), g_p.numel())
        dot_val = float(torch.dot(g_ref[:min_len], g_p[:min_len]).item()) if min_len > 0 else 0.0
        inner_sum += dot_val
        para_norm_sum += p_norm
        if use_cosine:
            cosine_sum += dot_val / (ref_norm * p_norm + 1e-12)

    n = max(1, len(paraphrases))
    align_inner = inner_sum / n
    align_cosine = (cosine_sum / n) if use_cosine else float("nan")
    return AlignOutputs(
        align_inner=align_inner,
        align_cosine=align_cosine,
        grad_norm_orig=float(ref_norm),
        grad_norm_para_mean=float(para_norm_sum / n),
    )


class GradientAlignmentAnalyzer:
    """Gradient Alignment Î∂ÑÏÑùÍ∏∞"""
    
    def __init__(self, 
                 use_cosine: bool = True,
                 fp16_autocast: bool = False,
                 only_last_n_layers: Optional[int] = None,
                 max_len: int = 512):
        self.use_cosine = use_cosine
        self.fp16_autocast = fp16_autocast
        self.only_last_n_layers = only_last_n_layers
        self.max_len = max_len
        
    def analyze_batch(self,
                     model: nn.Module,
                     tokenizer: AutoTokenizer,
                     original_texts: List[str],
                     paraphrase_texts_list: List[List[str]],
                     device: str = "cuda") -> Dict:
        """
        Î∞∞Ïπò Ï≤òÎ¶¨Î°ú Ïó¨Îü¨ ÏõêÎ¨∏Í≥º Ìå®Îü¨ÌîÑÎ†àÏù¥Ï¶àÏùò gradient alignment Í≥ÑÏÇ∞
        
        Args:
            model: Î∂ÑÏÑùÌï† Î™®Îç∏
            tokenizer: ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä
            original_texts: ÏõêÎ≥∏ ÌÖçÏä§Ìä∏ Î¶¨Ïä§Ìä∏
            paraphrase_texts_list: Í∞Å ÏõêÎ≥∏Ïóê ÎåÄÌïú Ìå®Îü¨ÌîÑÎ†àÏù¥Ï¶à Î¶¨Ïä§Ìä∏Îì§
            device: ÎîîÎ∞îÏù¥Ïä§
            
        Returns:
            Î∂ÑÏÑù Í≤∞Í≥º ÎîïÏÖîÎÑàÎ¶¨
        """
        results = {
            'alignments': [],
            'align_inner_scores': [],
            'align_cosine_scores': [],
            'gradient_norms': {
                'original': [],
                'paraphrase': []
            }
        }
        
        # head-only Í≤ΩÎ°úÏóêÏÑúÎäî params ÏÑúÎ∏åÏÖãÏù¥ ÌïÑÏöî ÏóÜÏùå
        model.eval()  # ÏùºÍ¥ÄÏÑ±ÏùÑ ÏúÑÌï¥ eval Î™®Îìú
        
        logger.info(f"Computing gradient alignments for {len(original_texts)} samples...")
        
        for i, (orig_text, paraphrases) in enumerate(zip(original_texts, paraphrase_texts_list)):
            if not paraphrases:  # Ìå®Îü¨ÌîÑÎ†àÏù¥Ï¶àÍ∞Ä ÏóÜÏúºÎ©¥ Ïä§ÌÇµ
                continue
                
            try:
                # Í∞úÎ≥Ñ alignment Í≥ÑÏÇ∞
                align_result = paraphrase_alignment_for_one(
                    model=model,
                    tokenizer=tokenizer,
                    orig_text=orig_text,
                    paraphrases=paraphrases,
                    max_len=self.max_len,
                    device=device,
                    use_cosine=self.use_cosine,
                    fp16_autocast=self.fp16_autocast
                )
                
                # Í≤∞Í≥º Ï†ÄÏû•
                results['alignments'].append(align_result)
                results['align_inner_scores'].append(align_result.align_inner)
                results['align_cosine_scores'].append(align_result.align_cosine)
                
                # Í∑∏ÎùºÎîîÏñ∏Ìä∏ ÎÖ∏Î¶Ñ Ï†ÄÏû• (Ïä§ÏπºÎùº Í∏∞Î∞ò)
                results['gradient_norms']['original'].append(align_result.grad_norm_orig)
                results['gradient_norms']['paraphrase'].append(align_result.grad_norm_para_mean)
                
                if (i + 1) % 5 == 0:
                    logger.debug(f"Processed {i + 1}/{len(original_texts)} samples")
                    
            except Exception as e:
                logger.warning(f"Failed to process sample {i}: {e}")
                continue
        
        # ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
        results['statistics'] = self._compute_statistics(results)
        
        return results
    
    def _compute_statistics(self, results: Dict) -> Dict:
        """Í≤∞Í≥º ÌÜµÍ≥Ñ Í≥ÑÏÇ∞"""
        inner_scores = [x for x in results['align_inner_scores'] if not torch.isnan(torch.tensor(x))]
        cosine_scores = [x for x in results['align_cosine_scores'] if not torch.isnan(torch.tensor(x))]
        
        stats = {
            'num_samples': len(results['alignments']),
            'mean_align_inner': sum(inner_scores) / max(len(inner_scores), 1),
            'std_align_inner': torch.tensor(inner_scores).std().item() if len(inner_scores) > 1 else 0.0,
            'mean_align_cosine': sum(cosine_scores) / max(len(cosine_scores), 1),
            'std_align_cosine': torch.tensor(cosine_scores).std().item() if len(cosine_scores) > 1 else 0.0,
            'mean_grad_norm_orig': sum(results['gradient_norms']['original']) / max(len(results['gradient_norms']['original']), 1),
            'mean_grad_norm_para': sum(results['gradient_norms']['paraphrase']) / max(len(results['gradient_norms']['paraphrase']), 1)
        }
        
        return stats
    
    def compare_models(self,
                      model1: nn.Module,
                      model2: nn.Module, 
                      tokenizer: AutoTokenizer,
                      original_texts: List[str],
                      paraphrase_texts_list: List[List[str]],
                      device: str = "cuda",
                      model1_name: str = "Model1",
                      model2_name: str = "Model2") -> Dict:
        """Îëê Î™®Îç∏ Í∞ÑÏùò gradient alignment ÎπÑÍµê"""
        
        logger.info(f"Comparing gradient alignments between {model1_name} and {model2_name}")
        
        # Í∞Å Î™®Îç∏Ïùò alignment Í≥ÑÏÇ∞
        results1 = self.analyze_batch(model1, tokenizer, original_texts, paraphrase_texts_list, device)
        results2 = self.analyze_batch(model2, tokenizer, original_texts, paraphrase_texts_list, device)
        
        # ÎπÑÍµê Í≤∞Í≥º
        comparison = {
            model1_name: results1,
            model2_name: results2,
            'comparison': {
                'inner_alignment_diff': results2['statistics']['mean_align_inner'] - results1['statistics']['mean_align_inner'],
                'cosine_alignment_diff': results2['statistics']['mean_align_cosine'] - results1['statistics']['mean_align_cosine'],
                'grad_norm_orig_diff': results2['statistics']['mean_grad_norm_orig'] - results1['statistics']['mean_grad_norm_orig'],
                'grad_norm_para_diff': results2['statistics']['mean_grad_norm_para'] - results1['statistics']['mean_grad_norm_para']
            }
        }
        
        return comparison


def create_alignment_report(analysis_results: Dict, title: str = "Gradient Alignment Analysis") -> str:
    """Î∂ÑÏÑù Í≤∞Í≥º Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±"""
    report = []
    report.append("=" * 60)
    report.append(title)
    report.append("=" * 60)
    
    if 'statistics' in analysis_results:
        stats = analysis_results['statistics']
        report.append(f"Samples processed: {stats['num_samples']}")
        report.append(f"Mean Inner Alignment: {stats['mean_align_inner']:.4f} (¬±{stats['std_align_inner']:.4f})")
        report.append(f"Mean Cosine Alignment: {stats['mean_align_cosine']:.4f} (¬±{stats['std_align_cosine']:.4f})")
        report.append(f"Mean Gradient Norm (Original): {stats['mean_grad_norm_orig']:.4f}")
        report.append(f"Mean Gradient Norm (Paraphrase): {stats['mean_grad_norm_para']:.4f}")
        report.append("")
        
        # Ìï¥ÏÑù
        cosine_align = stats['mean_align_cosine']
        if cosine_align > 0.7:
            report.append("üî• High gradient alignment - Strong memorization transfer detected")
        elif cosine_align > 0.3:
            report.append("‚ö†Ô∏è  Moderate gradient alignment - Partial memorization transfer") 
        else:
            report.append("‚úÖ Low gradient alignment - Weak memorization transfer")
    
    report.append("=" * 60)
    return "\n".join(report)
